{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773b1cee",
   "metadata": {},
   "source": [
    "### Load Packages\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd58b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7c21f",
   "metadata": {},
   "source": [
    "### 2 Reinforcement learning (RL)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6e12d",
   "metadata": {},
   "source": [
    "#### Question 1: (10 points) For visualization purpose, generate heat maps of Reward function 1 and Reward function 2. For the heat maps, make sure you display the coloring scale. You will have 2 plots for this question.\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045f526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "528fd757",
   "metadata": {},
   "source": [
    "### 3 Optimal policy learning using RL algorithms\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e0938",
   "metadata": {},
   "source": [
    "#### Question 2: (40 points) Create the environment of the agent using the information provided in section 2. To be specific, create the MDP by setting up the state-space, action set, transition probabilities, discount factor, and reward function. For creating the environment, use the following set of parameters:\n",
    "+ Number of states = 100 (state space is a 10 by 10 square grid as displayed in figure 1)\n",
    "+ Number of actions = 4 (set of possible actions is displayed in figure 2)\n",
    "+ w = 0.1\n",
    "+ Discount factor = 0.8\n",
    "+ Reward function 1\n",
    "#### After you have created the environment, then write an optimal state-value function that takes as input the environment of the agent and outputs the optimal value of each state in the grid. For the optimal state-value function, you have to implement the Initialization (lines 2-4) and Estimation (lines 5-13) steps of the Value Iteration algorithm. For the estimation step, use \u000f = 0.01. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal value of that state. In this part of question, you should have 1 plot. Let’s assume that your value iteration algorithm converges in N steps. Plot snapshots of state values in 5 different steps linearly distributed from 1 to N. Report N and your step numbers. What observations do you have from the plots?\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5458e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6a5f88a",
   "metadata": {},
   "source": [
    "#### Question 3: (5 points) Generate a heat map of the optimal state values across the 2-D grid. For generating the heat map, you can use the same function provided in the hint earlier (see the hint after question 1).\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b25da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6efd5054",
   "metadata": {},
   "source": [
    "#### Question 4: (15 points) Explain the distribution of the optimal state values across the 2-D grid. (Hint: Use the figure generated in question 3 to explain)\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d368485",
   "metadata": {},
   "source": [
    "#### Question 5: (20 points) Implement the computation step of the value iteration algorithm (lines 14-17) to compute the optimal policy of the agent navigating the 2-D state-space. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal action at that state. The optimal actions should be displayed using arrows. Does the optimal policy of the agent match your intuition? Please provide a brief explanation. Is it possible for the agent to compute the optimal action to take at each state by observing the optimal values of it’s neighboring states? In this question, you should have 1 plot.\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ff507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c57dc7f",
   "metadata": {},
   "source": [
    "#### Question 6: (10 points) Modify the environment of the agent by replacing Reward function 1 with Reward function 2. Use the optimal state-value function implemented in question 2 to compute the optimal value of each state in the grid. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal value of that state. In this question, you should have 1 plot.\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af88d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53570e39",
   "metadata": {},
   "source": [
    "#### Question 7: (20 points) Generate a heat map of the optimal state values (found in question 6) across the 2-D grid. For generating the heat map, you can use the same function provided in the hint earlier. Explain the distribution of the optimal state values across the 2-D grid. (Hint: Use the figure generated in this question to explain)\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdec7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466af0f8",
   "metadata": {},
   "source": [
    "#### Question 8: (20 points) Implement the computation step of the value iteration algorithm (lines 14-17) to compute the optimal policy of the agent navigating the 2-D state-space. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal action at that state. The optimal actions should be displayed using arrows. Does the optimal policy of the agent match your intuition? Please provide a brief explanation. In this question, you should have 1 plot.\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f396a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "074f9538",
   "metadata": {},
   "source": [
    "#### Question 9:(20 points) Change the hyper parameter w to 0.6 and find the optimal policy map similar to previous question for reward functions. Explain the differences you observe. What do you think about value of new w compared to previous value? Choose the w that you think give rise to better optimal policy and use that w for the next stages of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa9e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95d83c58",
   "metadata": {},
   "source": [
    "### 4 Inverse Reinforcement learning (IRL)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b734aed",
   "metadata": {},
   "source": [
    "#### Question 10: (10 points) Express c, x, D, b in terms of R, Pa, Pa1 , ti , u, λ and Rmax\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ea650",
   "metadata": {},
   "source": [
    "#### Question 11: (30 points) Sweep λ from 0 to 5 to get 500 evenly spaced values for λ. For each value of λ compute OA(s) by following the process described above. For this problem, use the optimal policy of the agent found in question 5 to fill in the OE(s) values. Then use equation 3 to compute the accuracy of the IRL algorithm for this value of λ. You need to repeat the above process for all 500 values of λ to get 500 data points. Plot λ (x-axis) against Accuracy (y-axis). In this question, you should have 1 plot.\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e66e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01a95877",
   "metadata": {},
   "source": [
    "#### Question 12: (5 points) Use the plot in question 11 to compute the value of λ for which accuracy is maximum. For future reference we will denote this value as λ(1)max. Please report λ(1)max.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a20ad29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14f537bc",
   "metadata": {},
   "source": [
    "#### Question 13: (15 points) For λ(1)max, generate heat maps of the ground truth reward and the extracted reward. Please note that the ground truth reward is the Reward function 1 and the extracted reward is computed by solving the linear program given by equation 2 with the λ parameter set to λ(1)max. In this question, you should have 2 plots.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69847eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcbe80fe",
   "metadata": {},
   "source": [
    "#### Question 14: (10 points) Use the extracted reward function computed in question 13, to compute the optimal values of the states in the 2-D grid. For computing the optimal values you need to use the optimal state-value function that you wrote in question 2. For visualization purpose, generate a heat map of the optimal state values across the 2-D grid (similar to the figure generated in question 3). In this question, you should have 1 plot.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5e5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cee50915",
   "metadata": {},
   "source": [
    "#### Question 15: (10 points) Compare the heat maps of Question 3 and Question 14 and provide a brief explanation on their similarities and differences.\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5796cd8",
   "metadata": {},
   "source": [
    "#### Question 16: (10 points) Use the extracted reward function found in question 13 to compute the optimal policy of the agent. For computing the optimal policy of the agent you need to use the function that you wrote in question 5. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal action at that state. The actions should be displayed using arrows. In this question, you should have 1 plot.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fc7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "831fbf20",
   "metadata": {},
   "source": [
    "#### Question 17: (10 points) Compare the figures of Question 5 and Question 16 and provide a brief explanation on their similarities and differences.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ff4eb",
   "metadata": {},
   "source": [
    "#### Question 18: (30 points) Sweep λ from 0 to 5 to get 500 evenly spaced values for λ. For each value of λ compute OA(s) by following the process described above. For this problem, use the optimal policy of the agent found in question 8 to fill in the OE(s) values. Then use equation 3 to compute the accuracy of the IRL algorithm for this value of λ. You need to repeat the above process for all 500 values of λ to get 500 data points. Plot λ (x-axis) against Accuracy (y-axis). In this question, you should have 1 plot.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9341b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d04a684",
   "metadata": {},
   "source": [
    "#### Question 19: (5 points) Use the plot in question 18 to compute the value of λ for which accuracy is maximum. For future reference we will denote this value as λ(2)max. Please report λ(2)max.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d243b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c225ac4",
   "metadata": {},
   "source": [
    "#### Question 20: (15 points) For λ(2)max, generate heat maps of the ground truth reward and the extracted reward. Please note that the ground truth reward is the Reward function 2 and the extracted reward is computed by solving the linear program given by equation 2 with the λ parameter set to λ(2)max. In this question, you should have 2 plots.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f8b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae54972f",
   "metadata": {},
   "source": [
    "#### Question 21: (10 points) Use the extracted reward function computed in question 20, to compute the optimal values of the states in the 2-D grid. For computing the optimal values you need to use the optimal state-value function that you wrote in question 2. For visualization purpose, generate a heat map of the optimal state values across the 2-D grid (similar to the figure generated in question 7). In this question, you should have 1 plot.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0a9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d28f852d",
   "metadata": {},
   "source": [
    "#### Question 22: (10 points) Compare the heat maps of Question 7 and Question 21 and provide a brief explanation on their similarities and differences.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b58e98",
   "metadata": {},
   "source": [
    "#### Question 23: (10 points) Use the extracted reward function found in question 20 to compute the optimal policy of the agent. For computing the optimal policy of the agent you need to use the function that you wrote in question 9. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal action at that state. The actions should be displayed using arrows. In this question, you should have 1 plot.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb19b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80f994ea",
   "metadata": {},
   "source": [
    "#### Question 24: (10 points) Compare the figures of Question 9 and Question 23 and provide a brief explanation on their similarities and differences.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a660e",
   "metadata": {},
   "source": [
    "#### Question 25: (50 points) From the figure in question 23, you should observe that the optimal policy of the agent has two major discrepancies. Please identify and provide the causes for these two discrepancies. One of the discrepancy can be fixed easily by a slight modification to the value iteration algorithm. Perform this modification and re-run the modified value iteration algorithm to compute the optimal policy of the agent. Also, recompute the maximum accuracy after this modification. Is there a change in maximum accuracy? The second discrepancy is harder to fix and is a limitation of the simple IRL algorithm.\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4f872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
